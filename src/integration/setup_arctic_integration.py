#!/usr/bin/env python3
"""
ARCTIC Dataset Integration for HaWoR
This script sets up ARCTIC dataset integration with HaWoR for supervised training and evaluation
"""

import os
import sys
import json
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Optional
import argparse

class ArcticIntegrationManager:
    """Manage ARCTIC dataset integration with HaWoR"""
    
    def __init__(self, hawor_root: str = ".", arctic_root: str = "./thirdparty/arctic"):
        self.hawor_root = Path(hawor_root)
        self.arctic_root = Path(arctic_root)
        self.supervised_data_root = self.hawor_root / "supervised_data" / "ARCTIC"
        
        print(f"üîß ARCTIC Integration Manager initialized")
        print(f"üìÅ HaWoR root: {self.hawor_root}")
        print(f"üìÅ ARCTIC root: {self.arctic_root}")
        print(f"üìÅ Supervised data root: {self.supervised_data_root}")
    
    def check_arctic_setup(self) -> Dict[str, bool]:
        """Check if ARCTIC is properly set up"""
        print("Checking ARCTIC setup...")
        
        checks = {
            'arctic_repo_exists': self.arctic_root.exists(),
            'arctic_docs_exist': (self.arctic_root / "docs").exists(),
            'arctic_scripts_exist': (self.arctic_root / "scripts_data").exists(),
            'arctic_bash_scripts_exist': (self.arctic_root / "bash").exists(),
            'credentials_set': self._check_credentials(),
            'data_downloaded': self._check_data_downloaded()
        }
        
        for check, status in checks.items():
            status_icon = "‚úÖ" if status else "‚ùå"
            print(f"  {status_icon} {check}: {status}")
        
        return checks
    
    def _check_credentials(self) -> bool:
        """Check if ARCTIC credentials are set"""
        required_vars = ['ARCTIC_USERNAME', 'ARCTIC_PASSWORD', 'SMPLX_USERNAME', 'SMPLX_PASSWORD', 'MANO_USERNAME', 'MANO_PASSWORD']
        
        for var in required_vars:
            if not os.environ.get(var):
                return False
        return True
    
    def _check_data_downloaded(self) -> bool:
        """Check if ARCTIC data is downloaded"""
        data_path = self.arctic_root / "data"
        if not data_path.exists():
            return False
        
        # Check for key directories
        required_dirs = ["arctic_data", "meta"]
        for dir_name in required_dirs:
            if not (data_path / dir_name).exists():
                return False
        
        return True
    
    def setup_credentials_guide(self):
        """Create a guide for setting up ARCTIC credentials"""
        guide_file = self.supervised_data_root / "ARCTIC_CREDENTIALS_SETUP.md"
        
        guide_content = f"""# ARCTIC Credentials Setup Guide

## Required Accounts

You need to register accounts on the following websites:

1. **ARCTIC Dataset**: https://arctic.is.tue.mpg.de/register.php
2. **SMPL-X**: https://smpl-x.is.tue.mpg.de/
3. **MANO**: https://mano.is.tue.mpg.de/

## Setting Up Credentials

After registering, export your credentials:

```bash
export ARCTIC_USERNAME=<YOUR_ARCTIC_EMAIL>
export ARCTIC_PASSWORD=<YOUR_ARCTIC_PASSWORD>
export SMPLX_USERNAME=<YOUR_SMPLX_EMAIL>
export SMPLX_PASSWORD=<YOUR_SMPLX_PASSWORD>
export MANO_USERNAME=<YOUR_MANO_EMAIL>
export MANO_PASSWORD=<YOUR_MANO_PASSWORD>
```

## Verify Credentials

Check if your credentials are set correctly:

```bash
echo $ARCTIC_USERNAME
echo $ARCTIC_PASSWORD
echo $SMPLX_USERNAME
echo $SMPLX_PASSWORD
echo $MANO_USERNAME
echo $MANO_PASSWORD
```

All should show your credentials (not empty).

## Next Steps

Once credentials are set, run:
```bash
python setup_arctic_integration.py --download-mini
```

This will download a small test dataset to verify everything works.
"""
        
        with open(guide_file, 'w') as f:
            f.write(guide_content)
        
        print(f"üìñ Created credentials guide: {guide_file}")
    
    def download_mini_arctic(self):
        """Download mini ARCTIC dataset for testing"""
        print("üöÄ Downloading mini ARCTIC dataset for testing...")
        
        if not self._check_credentials():
            print("‚ùå Credentials not set. Please set up credentials first.")
            self.setup_credentials_guide()
            return False
        
        # Change to ARCTIC directory
        original_cwd = os.getcwd()
        os.chdir(self.arctic_root)
        
        try:
            # Make scripts executable
            subprocess.run(['chmod', '+x', './bash/*.sh'], check=True)
            
            # Run dry run download
            print("üì• Running dry run download...")
            subprocess.run(['./bash/download_dry_run.sh'], check=True)
            
            # Unzip downloaded data
            print("üì¶ Unzipping downloaded data...")
            subprocess.run([sys.executable, 'scripts_data/unzip_download.py'], check=True)
            
            # Verify checksums
            print("Verifying checksums...")
            subprocess.run([sys.executable, 'scripts_data/checksum.py'], check=True)
            
            # Move unpacked data to expected location
            unpack_dir = self.arctic_root / "unpack"
            data_dir = self.arctic_root / "data"
            
            if unpack_dir.exists() and not data_dir.exists():
                shutil.move(str(unpack_dir), str(data_dir))
                print(f"‚úÖ Moved unpacked data to: {data_dir}")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Error during download: {e}")
            return False
        finally:
            os.chdir(original_cwd)
    
    def create_arctic_evaluation_script(self):
        """Create evaluation script for ARCTIC dataset"""
        eval_script = self.supervised_data_root / "evaluate_arctic_with_hawor.py"
        
        script_content = '''#!/usr/bin/env python3
"""
Evaluate HaWoR on ARCTIC dataset
"""

import os
import sys
import json
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List
import cv2
from tqdm import tqdm

# Add HaWoR to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from advanced_hawor import AdvancedHaWoR

class ArcticHaWoREvaluator:
    """Evaluate HaWoR on ARCTIC dataset"""
    
    def __init__(self, arctic_data_path: str, device: str = 'auto'):
        self.arctic_data_path = Path(arctic_data_path)
        self.device = device
        self.pipeline = AdvancedHaWoR(device=device)
        self.results = {}
        
        print(f"üîß ARCTIC-HaWoR Evaluator initialized")
        print(f"üìÅ ARCTIC data path: {self.arctic_data_path}")
        print(f"üîß Device: {device}")
    
    def find_arctic_sequences(self) -> List[Path]:
        """Find ARCTIC sequences to evaluate"""
        sequences = []
        
        # Look for processed sequences
        processed_dir = self.arctic_data_path / "arctic_data" / "data" / "splits"
        if processed_dir.exists():
            for split_dir in processed_dir.iterdir():
                if split_dir.is_dir():
                    for seq_file in split_dir.glob("*.npy"):
                        sequences.append(seq_file)
        
        # Look for raw sequences
        raw_dir = self.arctic_data_path / "arctic_data" / "data" / "raw_seqs"
        if raw_dir.exists():
            for subject_dir in raw_dir.iterdir():
                if subject_dir.is_dir():
                    for seq_file in subject_dir.glob("*.mano.npy"):
                        sequences.append(seq_file)
        
        return sequences
    
    def evaluate_sequence(self, seq_path: Path) -> Dict:
        """Evaluate a single ARCTIC sequence"""
        print(f"üé¨ Evaluating sequence: {seq_path.name}")
        
        try:
            # Load ARCTIC sequence data
            seq_data = np.load(seq_path, allow_pickle=True).item()
            
            # Extract relevant information
            analysis = {
                'sequence_name': seq_path.name,
                'sequence_path': str(seq_path),
                'arctic_data': {
                    'has_mano_params': 'mano' in seq_data,
                    'has_smplx_params': 'smplx' in seq_data,
                    'has_object_params': 'object' in seq_data,
                    'has_camera_params': 'cam' in seq_data,
                    'frame_count': len(seq_data.get('mano', {}).get('right', {}).get('global_orient', [])) if 'mano' in seq_data else 0
                }
            }
            
            # TODO: Add HaWoR evaluation logic here
            # This would involve:
            # 1. Converting ARCTIC data to HaWoR format
            # 2. Running HaWoR inference
            # 3. Comparing with ARCTIC ground truth
            # 4. Computing evaluation metrics
            
            return analysis
            
        except Exception as e:
            return {
                'sequence_name': seq_path.name,
                'sequence_path': str(seq_path),
                'error': str(e)
            }
    
    def run_evaluation(self):
        """Run evaluation on all ARCTIC sequences"""
        print("üöÄ Starting ARCTIC evaluation...")
        
        sequences = self.find_arctic_sequences()
        if not sequences:
            print("‚ö†Ô∏è  No ARCTIC sequences found")
            return {}
        
        print(f"üìä Found {len(sequences)} sequences to evaluate")
        
        results = {}
        for seq_path in tqdm(sequences, desc="Evaluating sequences"):
            result = self.evaluate_sequence(seq_path)
            results[seq_path.name] = result
        
        # Save results
        results_file = self.supervised_data_root / "arctic_evaluation_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"üìÑ Results saved to: {results_file}")
        return results

def main():
    """Main evaluation function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Evaluate HaWoR on ARCTIC dataset')
    parser.add_argument('--arctic-data', type=str, default='./thirdparty/arctic/data',
                       help='Path to ARCTIC data directory')
    parser.add_argument('--device', type=str, default='auto',
                       help='Device to use (cuda/mps/cpu/auto)')
    
    args = parser.parse_args()
    
    evaluator = ArcticHaWoREvaluator(args.arctic_data, args.device)
    evaluator.run_evaluation()

if __name__ == "__main__":
    main()
'''
        
        with open(eval_script, 'w') as f:
            f.write(script_content)
        
        # Make executable
        os.chmod(eval_script, 0o755)
        print(f"üìú Created ARCTIC evaluation script: {eval_script}")
    
    def create_arctic_training_script(self):
        """Create training script for ARCTIC dataset"""
        train_script = self.supervised_data_root / "train_hawor_on_arctic.py"
        
        script_content = '''#!/usr/bin/env python3
"""
Train HaWoR on ARCTIC dataset
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, List, Tuple
import cv2
from tqdm import tqdm

# Add HaWoR to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

class ArcticHaWoRTrainer:
    """Train HaWoR on ARCTIC dataset"""
    
    def __init__(self, arctic_data_path: str, device: str = 'auto'):
        self.arctic_data_path = Path(arctic_data_path)
        self.device = device
        self.results = {}
        
        print(f"üîß ARCTIC-HaWoR Trainer initialized")
        print(f"üìÅ ARCTIC data path: {self.arctic_data_path}")
        print(f"üîß Device: {device}")
    
    def create_arctic_dataloader(self):
        """Create dataloader for ARCTIC dataset"""
        # TODO: Implement ARCTIC dataloader
        # This would involve:
        # 1. Loading ARCTIC sequences
        # 2. Converting to HaWoR format
        # 3. Creating train/val/test splits
        # 4. Implementing data augmentation
        pass
    
    def train_hawor_on_arctic(self):
        """Train HaWoR model on ARCTIC dataset"""
        print("üöÄ Starting HaWoR training on ARCTIC...")
        
        # TODO: Implement training loop
        # This would involve:
        # 1. Loading pre-trained HaWoR model
        # 2. Setting up loss functions
        # 3. Training loop with ARCTIC data
        # 4. Validation and checkpointing
        pass

def main():
    """Main training function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train HaWoR on ARCTIC dataset')
    parser.add_argument('--arctic-data', type=str, default='./thirdparty/arctic/data',
                       help='Path to ARCTIC data directory')
    parser.add_argument('--device', type=str, default='auto',
                       help='Device to use (cuda/mps/cpu/auto)')
    
    args = parser.parse_args()
    
    trainer = ArcticHaWoRTrainer(args.arctic_data, args.device)
    trainer.train_hawor_on_arctic()

if __name__ == "__main__":
    main()
'''
        
        with open(train_script, 'w') as f:
            f.write(script_content)
        
        # Make executable
        os.chmod(train_script, 0o755)
        print(f"üìú Created ARCTIC training script: {train_script}")
    
    def create_integration_summary(self):
        """Create integration summary and next steps"""
        summary_file = self.supervised_data_root / "ARCTIC_INTEGRATION_SUMMARY.md"
        
        summary_content = f"""# ARCTIC Integration Summary

## What We've Set Up

‚úÖ **ARCTIC Repository**: Cloned from https://github.com/zc-alexfan/arctic
‚úÖ **Integration Scripts**: Created evaluation and training scripts
‚úÖ **Directory Structure**: Set up supervised data directories
‚úÖ **Documentation**: Created setup guides and instructions

## Current Status

- **ARCTIC Repository**: Available at `{self.arctic_root}` (in thirdparty/)
- **Credentials**: {'‚úÖ Set' if self._check_credentials() else '‚ùå Need to be set'}
- **Data Downloaded**: {'‚úÖ Downloaded' if self._check_data_downloaded() else '‚ùå Not downloaded'}

## Next Steps

### 1. Set Up Credentials (Required)
```bash
# Register accounts on:
# - https://arctic.is.tue.mpg.de/register.php
# - https://smpl-x.is.tue.mpg.de/
# - https://mano.is.tue.mpg.de/

# Then export credentials:
export ARCTIC_USERNAME=<YOUR_EMAIL>
export ARCTIC_PASSWORD=<YOUR_PASSWORD>
export SMPLX_USERNAME=<YOUR_EMAIL>
export SMPLX_PASSWORD=<YOUR_PASSWORD>
export MANO_USERNAME=<YOUR_EMAIL>
export MANO_PASSWORD=<YOUR_PASSWORD>
```

### 2. Download Mini Dataset (Test)
```bash
python setup_arctic_integration.py --download-mini
```

### 3. Download Full Dataset (When Ready)
```bash
cd thirdparty/arctic
./bash/download_cropped_images.sh  # 116GB - recommended for training
./bash/download_splits.sh          # 18GB - pre-processed splits
./bash/download_baselines.sh       # 6GB - pre-trained models
```

### 4. Run Evaluation
```bash
python supervised_data/ARCTIC/evaluate_arctic_with_hawor.py
```

### 5. Train HaWoR on ARCTIC
```bash
python supervised_data/ARCTIC/train_hawor_on_arctic.py
```

## ARCTIC Dataset Features

- **2.1M high-resolution images** with annotated frames
- **3D groundtruth for MANO** (perfect for HaWoR!)
- **Bimanual hand-object manipulation**
- **Egocentric view** (matches HaWoR's use case)
- **MoCap setup with 54 Vicon cameras**
- **Highly dexterous motion**

## Integration Benefits

1. **Rich Ground Truth**: ARCTIC provides high-quality MANO parameters
2. **Large Scale**: 2.1M images for robust training
3. **Realistic Scenarios**: Bimanual manipulation with objects
4. **Egocentric View**: Perfect for HaWoR's intended use case
5. **Established Benchmark**: Compare with state-of-the-art methods

## Files Created

- `ARCTIC_CREDENTIALS_SETUP.md` - Credentials setup guide
- `evaluate_arctic_with_hawor.py` - Evaluation script
- `train_hawor_on_arctic.py` - Training script
- `ARCTIC_INTEGRATION_SUMMARY.md` - This summary

## Notes

- ARCTIC requires registration on multiple websites
- Full dataset is ~1TB, start with mini dataset for testing
- Cropped images (116GB) are sufficient for most training needs
- Pre-processed splits are available for immediate use
"""
        
        with open(summary_file, 'w') as f:
            f.write(summary_content)
        
        print(f"üìÑ Created integration summary: {summary_file}")

def main():
    """Main integration function"""
    parser = argparse.ArgumentParser(description='ARCTIC Integration for HaWoR')
    parser.add_argument('--hawor-root', type=str, default='.',
                       help='HaWoR root directory')
    parser.add_argument('--arctic-root', type=str, default='./thirdparty/arctic',
                       help='ARCTIC repository directory')
    parser.add_argument('--download-mini', action='store_true',
                       help='Download mini ARCTIC dataset for testing')
    parser.add_argument('--check-setup', action='store_true',
                       help='Check ARCTIC setup status')
    
    args = parser.parse_args()
    
    print("üöÄ ARCTIC Integration for HaWoR")
    print("="*50)
    
    # Initialize manager
    manager = ArcticIntegrationManager(args.hawor_root, args.arctic_root)
    
    # Create supervised data directory
    manager.supervised_data_root.mkdir(parents=True, exist_ok=True)
    
    if args.check_setup:
        manager.check_arctic_setup()
    
    if args.download_mini:
        success = manager.download_mini_arctic()
        if success:
            print("‚úÖ Mini ARCTIC dataset downloaded successfully!")
        else:
            print("‚ùå Failed to download mini ARCTIC dataset")
    
    # Always create integration files
    manager.setup_credentials_guide()
    manager.create_arctic_evaluation_script()
    manager.create_arctic_training_script()
    manager.create_integration_summary()
    
    print("\\n" + "="*50)
    print("‚úÖ ARCTIC Integration Setup Complete!")
    print("="*50)
    print("üìã Next steps:")
    print("  1. Set up credentials (see ARCTIC_CREDENTIALS_SETUP.md)")
    print("  2. Download mini dataset: python setup_arctic_integration.py --download-mini")
    print("  3. Run evaluation: python supervised_data/ARCTIC/evaluate_arctic_with_hawor.py")
    print("\\nüéØ ARCTIC is perfect for HaWoR - 2.1M images with MANO ground truth!")

if __name__ == "__main__":
    main()
