#!/usr/bin/env python3
"""
Data Preparation Script for HaWoR Training
Auto-generated by setup_training_pipeline.py
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from training_data_preparation import ArcticDataConverter

def main():
    """Prepare training data from ARCTIC dataset"""
    print("üöÄ Starting data preparation for HaWoR training...")

    # Configuration
    arctic_root = "thirdparty/arctic/unpack/arctic_data/data"
    training_output = "training_data"
    validation_output = "validation_data"

    # Initialize converter
    converter = ArcticDataConverter(
        arctic_root=arctic_root,
        output_dir=training_output,
        target_resolution=(256, 256),
        num_workers=8
    )

    # Convert training data (first 80% of sequences)
    print("\n1Ô∏è‚É£  Converting training data...")
    train_subjects = ["s01", "s02", "s04", "s05", "s06", "s07"]
    train_stats = converter.convert_dataset(
        subjects=train_subjects,
        max_sequences_per_subject=15,
        output_split="train"
    )
    print(f"Training data conversion completed: {train_stats}")

    # Convert validation data (remaining 20%)
    print("\n2Ô∏è‚É£  Converting validation data...")
    converter.output_dir = validation_output
    val_subjects = ["s08", "s09", "s10"]
    val_stats = converter.convert_dataset(
        subjects=val_subjects,
        max_sequences_per_subject=5,
        output_split="val"
    )
    print(f"Validation data conversion completed: {val_stats}")

    print("\n‚úÖ Data preparation completed successfully!")
    print(f"Training data: {training_output}")
    print(f"Validation data: {validation_output}")

if __name__ == "__main__":
    main()
